{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis & Visualization of Produced Water Chemistry for Environmental & Agricultural Utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will all probably need to do this first, so putting this temporary code cell here as a reminder\n",
    "# once everyone has it installed, this cell should be deleted\n",
    "!pip install wqchartpy\n",
    "# if pip install doesn't work for you, go to https://github.com/jyangfsu/WQChartPy/tree/main and follow the \"another way\" instructions\n",
    "# if you had to use the \"another way\" method, you will have to restart your kernal after finishing the install in order for the import to work\n",
    "#This is my test4 yay\n",
    "#Sarah comment\n",
    "#Sarah comment number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from wqchartpy import triangle_piper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING & FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data from the CSV files\n",
    "df1 = pd.read_csv('../data/split_1_USGSPWDBv2.3n.csv', low_memory=False)\n",
    "df2 = pd.read_csv('../data/split_2_USGSPWDBv2.3n.csv', low_memory=False)\n",
    "df3 = pd.read_csv('../data/split_3_USGSPWDBv2.3n.csv', low_memory=False)\n",
    "\n",
    "# Concatenate the dataframes\n",
    "frames = [df1, df2, df3]\n",
    "df_merged = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Save the concatenated dataframe to a new CSV file\n",
    "df_merged.to_csv('../data/df_merged.csv', index=False)\n",
    "\n",
    "# List of columns to be removed\n",
    "columns_to_remove = [\n",
    "    \"IDDB\", \"SOURCE\", \"REFERENCE\", \"LATLONGAPX\", \"USGSREGION\", \"BASINCODE\", \n",
    "    \"STATECODE\", \"COUNTYCODE\", \"FIELD\", \"FIELDCODE\", \"WELLCODE\", \"TOWNRANGE\", \n",
    "    \"REGDIST\", \"LOC\", \"QUAD\", \"DAY\", \"DATECOMP\", \"DATEANALYS\", \"METHOD\", \n",
    "    \"OPERATOR\", \"PERMIT\", \"DFORM\", \"GROUP\", \"MEMBER\", \"AGECODE\", \"ERA\", \n",
    "    \"PERIOD\", \"EPOCH\", \"LAB\", \"REMARKS\", \"LITHOLOGY\", \"POROSITY\", \"TEMP\", \n",
    "    \"PRESSURE\", \"SG\", \"SPGRAV\", \"SPGRAVT\", \"RESIS\", \"RESIST\", \"PH\", \"PHT\", \n",
    "    \"EHORP\", \"COND\", \"CONDT\", \"TURBIDITY\", \"HEM\", \"MBAS\",\"TDS\",\"TDSCALC\", \"TSS\", \"CHARGEBAL\", \n",
    "    \"ACIDITY\", \"DIC\", \"DOC\", \"TOC\", \"CN\", \"BOD\", \"COD\", \"BENZENE\", \"TOLUENE\", \n",
    "    \"ETHYLBENZ\", \"XYLENE\", \"ACETATE\", \"BUTYRATE\", \"FORMATE\", \"LACTATE\", \n",
    "    \"PHENOLS\", \"PERC\", \"PROPIONATE\", \"PYRUVATE\", \"VALERATE\", \"ORGACIDS\", \n",
    "    \"Ar\", \"CH4\", \"C2H6\", \"CO2\", \"H2\", \"H2S\", \"He\", \"N2\", \"NH3\", \"O2\", \"ALPHA\", \n",
    "    \"BETA\", \"dD\", \"H3\", \"d7Li\", \"d11B\", \"d13C\", \"C14\", \"d18O\", \"d34S\", \n",
    "    \"d37Cl\", \"K40\", \"d81Br\", \"Sr87Sr86\", \"I129\", \"Rn222\", \"Ra226\", \"Ra228\", \n",
    "    \"cull_PH\", \"cull_MgCa\", \"cull_KCl\", \"cull_K5Na\", \"Ag\", \"Al\", \"As\", \"Au\", \n",
    "    \"B\", \"BO3\", \"Be\", \"Bi\", \"Cd\", \"Co\", \"Cr\", \"Cs\", \"Cu\", \"F\", \"FeS\", \"FeAl\", \n",
    "    \"FeAl2O3\", \"Hg\", \"I\", \"Mn\", \"Mo\", \"N\", \"NO2\", \"NO3\", \"NO3NO2\", \"NH4\", \n",
    "    \"TKN\", \"Ni\", \"OH\", \"P\", \"PO4\", \"Pb\", \"Rh\", \"Rb\", \"S\", \"SO3\", \"HS\", \"Sb\", \n",
    "    \"Sc\", \"Se\", \"Sn\", \"Ti\", \"Tl\", \"U\", \"V\", \"W\", \"Zn\"\n",
    "]\n",
    "\n",
    "# Remove the specified columns\n",
    "df_limited_column = df_merged.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "# Display the updated merged dataframe\n",
    "df_limited_column.to_csv('../data/df_limited_column.csv', index=False)\n",
    "\n",
    "# Remove rows where TDSUSGS <= 35000 (sea water to eliminate all coalebed methane produced water and also the failing analyses)\n",
    "df_filtered = df_limited_column[df_limited_column['TDSUSGS'] > 35000]\n",
    "\n",
    "# Save the filtered dataframe to a new CSV file\n",
    "df_filtered.to_csv('../data/df_filtered_TDS.csv', index=False)\n",
    "\n",
    "\n",
    "# Fill NaN values in 'KNa', 'K', 'Na', 'Ca', 'Cl', 'SO4', 'Mg' with zeros for calculation\n",
    "for col in ['KNa', 'K', 'Na', 'Ca', 'Cl', 'SO4', 'Mg']:\n",
    "    df_filtered[col].fillna(0, inplace=True)\n",
    "\n",
    "# First, we'll fill NaN values in 'KNa' and 'K' with zeros for the calculation.\n",
    "df_filtered['KNa'].fillna(0, inplace=True)\n",
    "df_filtered['K'].fillna(0, inplace=True)\n",
    "\n",
    "# Apply conditions to calculate 'Na'\n",
    "# If 'Na' is missing and both 'KNa' and 'K' are present, populate 'Na' with 'KNa' - 'K'\n",
    "# If 'Na' is missing and 'KNa' is present but 'K' is not, populate 'Na' with 'KNa'\n",
    "\n",
    "na_mask = df_filtered['Na'].isna()\n",
    "na_present = df_filtered['Na'] > 0\n",
    "kna_present = df_filtered['KNa'] > 0\n",
    "k_present = df_filtered['K'] > 0\n",
    "k_missing = df_filtered['K'] == 0\n",
    "\n",
    "df_filtered.loc[k_missing & na_present & kna_present, 'K'] = df_filtered['KNa'] - df_filtered['Na']\n",
    "df_filtered.loc[na_mask & kna_present & k_present, 'Na'] = df_filtered['KNa'] - df_filtered['K']\n",
    "df_filtered.loc[na_mask & kna_present & ~k_present, 'Na'] = df_filtered['KNa']\n",
    "\n",
    "# Remove rows where 'Na' is still missing\n",
    "df_filtered.dropna(subset=['Na'], inplace=True)\n",
    "\n",
    "# Remove rows where 'Cl' is missing\n",
    "df_filtered.dropna(subset=['Cl'], inplace=True)\n",
    "\n",
    "# Save the updated dataframe \n",
    "df_filtered.to_csv('../data/df_filtered_Na_Cl.csv', index=False)\n",
    "\n",
    "#To calculate the molar concentrations from concentrations given in ppm (parts per million) or mg/L,\n",
    "#these values need to be converted into moles per liter (M). The formula to convert ppm or mg/L to M is:\n",
    "\n",
    "                #Molarity (M)=Concentration (mg/L)/ Molar Mass (g/mol) \n",
    "\n",
    "#This calculation assumes that 1 ppm is equivalent to 1 mg/L. The molar mass of each element or compound (Na, Ca, Cl, SO4, and Mg) \n",
    "#is a constant value based on its atomic or molecular weight.\n",
    "\n",
    "#Apply the conditions (molar Na > molar Ca) and (molar Cl > molar SO4) and (molar Ca > molar Mg/2) which represent likely unnatural combinations\n",
    "# Convert concentrations from ppm (mg/L) to Molarity (M)\n",
    "molar_masses = {'Na': 22.99, 'Ca': 40.08, 'Cl': 35.45, 'SO4': 96.06, 'Mg': 24.305}\n",
    "for element, molar_mass in molar_masses.items():\n",
    "    df_filtered[element + '_M'] = df_filtered[element] / molar_mass\n",
    "\n",
    "# Apply the conditions (molar Na > molar Ca) and (molar Cl > molar SO4) and (molar Ca > molar Mg/2)\n",
    "condition = (df_filtered['Na_M'] > df_filtered['Ca_M']) & \\\n",
    "            (df_filtered['Cl_M'] > df_filtered['SO4_M']) & \\\n",
    "            (df_filtered['Ca_M'] > df_filtered['Mg_M'] / 2)\n",
    "\n",
    "\n",
    "df_filtered = df_filtered[condition]\n",
    "\n",
    "# Save the updated dataframe\n",
    "df_filtered.to_csv('../data/df_filtered_corrected_elemental_ratios.csv', index=False)\n",
    "\n",
    "# Filter out rows where USGS charge balance is not between -10 and +10\n",
    "df_filtered = df_filtered[df_filtered['chargebalance'].between(-10, 10)]\n",
    "\n",
    "# Save the updated dataframe\n",
    "df_filtered.to_csv('../data/df_filtered_chargebalance.csv', index=False)\n",
    "\n",
    "# Calculate charge balance\n",
    "df_filtered['Cations'] = (df_filtered['Na_M'] * 1) + (df_filtered['Ca_M'] * 2) + (df_filtered['Mg_M'] * 2)\n",
    "df_filtered['Anions'] = (df_filtered['Cl_M'] * 1) + (df_filtered['SO4_M'] * 2)\n",
    "df_filtered['CalculatedChargeBalance'] = ((df_filtered['Cations'] - df_filtered['Anions']) / (df_filtered['Cations'] + df_filtered['Anions'])) * 100\n",
    "\n",
    "# Flag discrepancies between calculated charge balance and existing 'chargebalance' column\n",
    "threshold = 5  # 5% threshold for discrepancy\n",
    "df_filtered['ChargeBalanceDiscrepancy'] = abs(df_filtered['CalculatedChargeBalance'] - df_filtered['chargebalance']) > threshold\n",
    "\n",
    "# Save the updated dataframe with discrepancy flags\n",
    "df_filtered.to_csv('../data/df_filtered_discrepancy_flags.csv', index=False)\n",
    "\n",
    "# Calculate DEPTHWELL where it's missing\n",
    "df_filtered_depth=df_filtered.copy()\n",
    "\n",
    "for index, row in df_filtered_depth.iterrows():\n",
    "    if pd.isna(row['DEPTHWELL']):\n",
    "        if pd.notna(row['DEPTHUPPER']) and pd.notna(row['DEPTHLOWER']):\n",
    "            # Calculate DEPTHWELL as difference between DEPTHUPPER and DEPTHLOWER\n",
    "            df_filtered_depth.at[index, 'DEPTHWELL'] = row['DEPTHLOWER'] - row['DEPTHUPPER']\n",
    "        elif pd.notna(row['DEPTHUPPER']):\n",
    "            # Populate DEPTHWELL with DEPTHUPPER\n",
    "            df_filtered_depth.at[index, 'DEPTHWELL'] = row['DEPTHUPPER']\n",
    "        elif pd.notna(row['DEPTHLOWER']):\n",
    "            # Populate DEPTHWELL with DEPTHLOWER\n",
    "            df_filtered_depth.at[index, 'DEPTHWELL'] = row['DEPTHLOWER']\n",
    "\n",
    "# Remove rows where DEPTHWELL, DEPTHUPPER, and DEPTHLOWER are all missing\n",
    "df_filtered_depth.dropna(subset=['DEPTHWELL'], inplace=True)\n",
    "\n",
    "# Save the updated dataframe with updated well depth\n",
    "df_filtered_depth.to_csv('../data/df_filtered_welldepth.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive Statistics\n",
    "\n",
    "descriptive_stats = df_filtered.describe()\n",
    "descriptive_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive Statistics\n",
    "\n",
    "specific_columns_stats = df_filtered[['Li','Na', 'Mg','Ca', 'Cl', 'SO4']].describe()\n",
    "specific_columns_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal Component Analysis (PCA)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_columns = df_filtered.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Check for columns with all NaN values and drop them (if necessary)\n",
    "df_filtered = df_filtered.dropna(axis=1, how='all')\n",
    "\n",
    "# Replacing NaN values with the min of the column\n",
    "df_filtered2=df_filtered.fillna(df_filtered.min())\n",
    "\n",
    "# Check and handle infinity values\n",
    "df_filtered2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_filtered2.fillna(df_filtered.mean(), inplace=True)\n",
    "\n",
    "\n",
    "# Standardize the data: PCA is affected by scale so we need to scale the features in our data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_filtered2[numerical_columns])\n",
    "\n",
    "# Initialize PCA - uses 2 components for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame with the PCA results\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PCA1', 'PCA2'])\n",
    "\n",
    "# Plotting the PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_df['PCA1'], pca_df['PCA2'], alpha=0.5)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Dataset')\n",
    "plt.show()\n",
    "\n",
    "# Display the amount of variance explained by each component\n",
    "print(f\"Explained variance by component: {pca.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster Analysis\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "columns_for_clustering = ['BASIN', 'Li']\n",
    "\n",
    "basins = df_filtered['BASIN'].unique()\n",
    "# Create a figure and a set of subplots\n",
    "fig, axes = plt.subplots(nrows=len(basins), ncols=1, figsize=(10, 5 * len(basins)))\n",
    "\n",
    "for i, basin in enumerate(basins):\n",
    "    subset = df_filtered[df_filtered['BASIN'] == basin]\n",
    "    \n",
    "# Check if the subset has enough samples for clustering\n",
    "if len(subset) >= 3:\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    clusters = kmeans.fit_predict(subset[['Li']])\n",
    "    df_filtered.loc[df_filtered['BASIN'] == basin, 'cluster'] = clusters\n",
    "\n",
    "    # Plot each basin's clusters\n",
    "    ax = axes[i] if len(basins) > 1 else axes\n",
    "    ax.scatter(subset['Li'], [i]*len(subset), c=clusters, cmap='viridis', label=basin)\n",
    "    ax.set_title(f'Clusters in {basin} Basin')\n",
    "    ax.set_xlabel('Lithium Concentration')\n",
    "    ax.set_yticks([])\n",
    "    ax.legend()\n",
    "else:\n",
    "    # Handle basins with too few samples (e.g., by skipping or plotting without clustering)\n",
    "    ax = axes[i] if len(basins) > 1 else axes\n",
    "    ax.scatter(subset['Li'], [i]*len(subset), label=basin)\n",
    "    ax.set_title(f'{basin} Basin (Insufficient Data for Clustering)')\n",
    "    ax.set_xlabel('Lithium Concentration')\n",
    "    ax.set_yticks([])\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Analysis\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "# Convert 'DATESAMPLE' to datetime\n",
    "df_filtered['DATESAMPLE'] = pd.to_datetime(df_filtered['DATESAMPLE'])\n",
    "\n",
    "# Group by 'WELLNAME' and filter for groups with more than three unique samples\n",
    "groups = df_filtered.groupby('WELLNAME')\n",
    "well_groups = {well: group for well, group in groups if group['DATESAMPLE'].nunique() > 3}\n",
    "\n",
    "# Decide on a reasonable number of plots per figure\n",
    "plots_per_figure = 10  # Adjust this number based on your preference and screen size\n",
    "num_wells = len(well_groups)\n",
    "num_figures = math.ceil(num_wells / plots_per_figure)\n",
    "\n",
    "# Loop through each subset of wells and create a figure for each\n",
    "for fig_idx in range(num_figures):\n",
    "    start_idx = fig_idx * plots_per_figure\n",
    "    end_idx = start_idx + plots_per_figure\n",
    "    current_wells = list(well_groups.keys())[start_idx:end_idx]\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(nrows=min(plots_per_figure, len(current_wells)), ncols=1, figsize=(10, 5 * min(plots_per_figure, len(current_wells))))\n",
    "    if plots_per_figure > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax_idx, well in enumerate(current_wells):\n",
    "        data = well_groups[well].drop_duplicates(subset='DATESAMPLE', keep='first')\n",
    "        data = data.sort_values(by='DATESAMPLE')\n",
    "\n",
    "        ax = axes[ax_idx]\n",
    "        ax.plot(data['DATESAMPLE'], data['Na'], marker='o')\n",
    "        ax.set_title(f'Time Series for {well}')\n",
    "        ax.set_xlabel('Sampling Date')\n",
    "        ax.set_ylabel('Sodium Concentration')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Hypothesis: Li Concentration vs TDS\n",
    "# Check for NaN values and remove them\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "data = df_filtered[['Li', 'TDSUSGS']].dropna()\n",
    "\n",
    "# Perform Spearman's rank correlation test\n",
    "spearman_corr, spearman_pval = stats.spearmanr(data['Li'], data['TDSUSGS'])\n",
    "\n",
    "print(\"Spearman's rank correlation test between Li concentration and TDS:\")\n",
    "print(\"Correlation Coefficient:\", spearman_corr)\n",
    "print(\"P-value:\", spearman_pval)\n",
    "\n",
    "# Interpret the results\n",
    "if spearman_pval < 0.05:\n",
    "    print(\"There is a statistically significant relationship between Li concentration and TDS.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant relationship between Li concentration and TDS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Test Hypothesis: Li Concentration vs Depth\n",
    "    \n",
    "import scipy.stats as stats\n",
    "    \n",
    "# Check for NaN values and remove them\n",
    "data = df_filtered[['Li', 'DEPTHUPPER']].dropna()\n",
    "\n",
    "# Perform Spearman's rank correlation test\n",
    "spearman_corr, spearman_pval = stats.spearmanr(data['Li'], data['DEPTHUPPER'])\n",
    "\n",
    "print(\"Spearman's rank correlation test between Li concentration and Depth:\")\n",
    "print(\"Correlation Coefficient:\", spearman_corr)\n",
    "print(\"P-value:\", spearman_pval)\n",
    "\n",
    "# Interpret the results\n",
    "if spearman_pval < 0.05:\n",
    "    print(\"There is a statistically significant relationship between Li concentration and Depth.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant relationship between Li concentration and Depth.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Analysis\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Remove rows where either Li or DEPTWELL is NaN for regression model training\n",
    "data_for_regression = df_filtered.dropna(subset=['Li', 'DEPTHWELL'])\n",
    "\n",
    "# Known Li values\n",
    "X_known = data_for_regression['Li'] \n",
    "y_known = data_for_regression['DEPTHWELL']  \n",
    "\n",
    "# Rows where Li is unknown\n",
    "unknown_li = df_filtered[df_filtered['Li'].isna() & df_filtered['DEPTHWELL'].notna()]\n",
    "X_unknown = unknown_li['Li'] \n",
    "y_unknown=unknown_li['DEPTHWELL']\n",
    "\n",
    "# Check for NaN values in X_unknown and replace with 0\n",
    "X_unknown.fillna(0, inplace=True)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model with known Li values\n",
    "model.fit(X_known.values.reshape(-1, 1), y_known.values)  \n",
    "\n",
    "# Predict Li for known and unknown DEPTHWELL values\n",
    "X_pred_known = model.predict(y_known.values.reshape(-1, 1))\n",
    "X_pred_unknown = model.predict(y_unknown.values.reshape(-1, 1))\n",
    "\n",
    "# Plotting Li vs DEPTHUPPER with regression line and predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot known Li values\n",
    "plt.scatter(X_known, y_known, color='blue', alpha=0.5, label='Known Li Values')\n",
    "\n",
    "# Plot predicted Li values for known DEPTH\n",
    "plt.scatter(X_unknown, y_pred, marker='x', color='red', alpha=0.5, label='Predicted Li on Known Data')\n",
    "\n",
    "# Plot predicted Li values for unknown DEPTH\n",
    "#plt.scatter(X_unknown, y_pred_unknown, color='green', alpha=0.5, label='Predicted Li on Unknown Data')\n",
    "\n",
    "# Plot regression line\n",
    "li_range = np.linspace(X_known.min(), X_known.max(), 100)\n",
    "depth_regression = model.predict(li_range.reshape(-1, 1))\n",
    "plt.plot(li_range, depth_regression, color='black', linestyle='-', label='Regression Line')\n",
    "\n",
    "plt.xlabel('Li Concentration')\n",
    "plt.ylabel('Depth')\n",
    "plt.title('Li Concentration vs Depth with Regression Line')\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.gca().invert_yaxis() \n",
    "plt.legend()\n",
    "\n",
    "# Add regression formula to the plot\n",
    "plt.text(0.1, 0.9, f'Depth = {model.intercept_:.2f} + {model.coef_[0]:.2f} * Li',\n",
    "         transform=plt.gca().transAxes, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from wqchartpy import triangle_piper\n",
    "from wqchartpy import contour_piper\n",
    "from wqchartpy import color_piper\n",
    "\n",
    "    \n",
    "# The below list of column names is just to document the required column names and column order to feed into wqchartpy - variable is not used; just here for reference\n",
    "columns = ['Sample','Label','Marker','Size','Color','Alpha','Ca','Mg','Na','K','HCO3','CO3','Cl','SO4']\n",
    "\n",
    "\n",
    "# Create new dataframe to match format required by wqchartpy for the contour piper plot\n",
    "df_data_wqchartpyformat = pd.DataFrame()\n",
    "\n",
    "df_data_wqchartpyformat = pd.DataFrame()\n",
    "df_data_wqchartpyformat['Sample'] = df_filtered['IDUSGS'].map(str)\n",
    "df_data_wqchartpyformat['Label'] = 'sample'       # DECISION NEEDED --- we can get fancy with how we want to group this? maybe group it by TDS high/med/low, etc? just have single group for all for now -- only a factor if we use the normal triangle_piper\n",
    "df_data_wqchartpyformat['Marker'] = 'o'           # DECISION NEEDED --- we can get fancy with how we want to identify markers? maybe group it by TDS high/med/low, etc? just have single shape for all for now -- only a factor if we use the normal triangle_piper\n",
    "df_data_wqchartpyformat['Color'] = '#FFFF00'      # DECISION NEEDED --- we can get fancy with how we want to color this? maybe group it by TDS high/med/low, etc? just have single color for all for now -- only a factor if we use the normal triangle_piper\n",
    "df_data_wqchartpyformat['Size'] = 10\n",
    "df_data_wqchartpyformat['Alpha'] = 0.6\n",
    "\n",
    "df_data_wqchartpyformat['Ca'] = df_filtered['Ca']\n",
    "df_data_wqchartpyformat['Mg'] = df_filtered['Mg']\n",
    "df_data_wqchartpyformat['Na'] = df_filtered['Na']\n",
    "df_data_wqchartpyformat['K'] = df_filtered['K']\n",
    "df_data_wqchartpyformat['HCO3'] = df_filtered['HCO3']\n",
    "df_data_wqchartpyformat['CO3'] = df_filtered['CO3']\n",
    "df_data_wqchartpyformat['Cl'] = df_filtered['Cl']\n",
    "df_data_wqchartpyformat['SO4'] = df_filtered['SO4']\n",
    "\n",
    "# Reset the index\n",
    "df_data_wqchartpyformat.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Show the df\n",
    "df_data_wqchartpyformat.head(2)\n",
    "\n",
    "image_name = 'TestPiper1-Normal'\n",
    "triangle_piper.plot(df_data_wqchartpyformat,unit='mg/L',figname=image_name,figformat='png')\n",
    "triangle_piper.show()\n",
    "\n",
    "\n",
    "#image_name = 'TestPiper2-Contour'\n",
    "#contour_piper.plot(df_data_wqchartpyformat, unit='mg/L', figname=image_name, figformat='png')\n",
    "\n",
    "#image_file_name = f'{image_name}.png'\n",
    "#image_folder_name = 'images'\n",
    "#move_wqchartpy_image_file_to_images_folder(image_file_name, image_folder_name)\n",
    "\n",
    "#image_name = 'TestPiper3-ColorCoded'\n",
    "#color_piper.plot(df_data_wqchartpyformat, unit='mg/L', figname=image_name, figformat='png')\n",
    "\n",
    "#image_file_name = f'{image_name}.png'\n",
    "#image_folder_name = 'images'\n",
    "#move_wqchartpy_image_file_to_images_folder(image_file_name, image_folder_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "chemicals = ['Na', 'Cl', 'SO4', 'Mg']\n",
    "\n",
    "# Calculate the correlation matrix using Pearson coefficient\n",
    "correlation_matrix_pearson = df_filtered[chemicals].corr(method='pearson')\n",
    "\n",
    "# Alternatively, for Spearman coefficient\n",
    "# correlation_matrix_spearman = df_filtered[chemicals].corr(method='spearman')\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix_pearson, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
    "plt.title('Correlation Matrix (Pearson) for Chemical Constituents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
